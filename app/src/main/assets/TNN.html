<p><a class="visually-hidden focusable skip-link" href="https://builtin.com/artificial-intelligence/transformer-neural-network#main-content">Skip to main content</a></p>
<div class="dialog-off-canvas-main-canvas" data-off-canvas-main-canvas="">
<div class="layout-container">
<div class="header header-vue builtin-nuxt-app">
<div class="region region-header">
<div id="block-mainbuiltinnavigation" class="block block-bix-global block-bix-global-vue-navigation">
<div id="__nuxt">
<div id="__layout">
<div class="nav-subapp-wrapper" data-v-26e09b2a="">
<div id="builtin-navigation" class="builtin-navigation-wrapper" data-v-26e09b2a=""><nav id="app-navigation" class="banner position-static" data-v-7360640c="" data-v-26e09b2a="">
<div class="position-relative" data-v-7360640c="">
<div class="position-relative" data-v-7360640c="">
<div class="bg-main-nav" data-v-7360640c="">
<div class="grid" data-v-7360640c="">
<div class="center-column" data-v-7360640c="">
<div class="column-left" data-v-7360640c="">
<div class="logo navigation-logo" data-v-36c7a25e="" data-v-7360640c=""><a href="https://builtin.com/" data-v-1734c926="" data-v-36c7a25e=""><img class="desktop visible" src="https://frontend.builtin.com/be3e4267b15558dc2f203a1b2645aef5248ad013/assets/site-logos/9.svg" alt="Built In National" width="138" height="49" data-v-29970e71="" data-v-36c7a25e="" data-v-1734c926="" /><span class="sr-only" data-v-36c7a25e="" data-v-1734c926="">Built In National</span></a></div>
</div>
<div class="column-right" data-v-7360640c="">
<div class="menu-container menu-type-main" data-v-ac30a42e="" data-v-7360640c=""><button class="menu-search" type="button" data-v-ac30a42e=""><span class="sr-only" data-v-ac30a42e="">Open Search</span></button>
<div class="menu-item" data-v-ac30a42e="">
<div data-v-07caf33e="" data-v-ac30a42e=""><a class="employers" href="https://employers.builtin.com/membership?utm_medium=BIReferral&amp;utm_source=foremployers" target="_blank" data-v-1734c926="" data-v-07caf33e="">FOR EMPLOYERS</a></div>
</div>
<div data-v-ac30a42e="">
<div class="profile-auth" data-v-dcdff77e="" data-v-ac30a42e="">
<div id="g_id_onload" data-v-dcdff77e="" data-client_id="941473408521-r82mlse6rh2ar9loff8bgnbqtbp6inrs.apps.googleusercontent.com" data-login_uri="https://accounts.builtin.com/External/OneTap" data-new_user_return_url="https://builtin.com/onboarding" data-existing_user_return_url="https://builtin.com/login?destination=%2Fartificial-intelligence%2Ftransformer-neural-network" data-prompt_parent_id="g_id_onload">&nbsp;</div>
<button class="b-oauth signup" data-v-dcdff77e="">JOIN</button><button class="b-oauth login" data-v-dcdff77e="">LOG IN</button></div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="menu-wrapper menu menu-desktop navigation-secondary" data-v-6961c67e="" data-v-7360640c="">
<ul class="menu-container grid d-flex" data-v-6961c67e="">
<li class="menu-item" tabindex="0" data-v-6961c67e=""><a class="menu-link bix_link nav-jobs" href="https://builtin.com/jobs" data-v-1734c926="" data-v-6961c67e="">JOBS&nbsp;</a></li>
<li class="menu-item" tabindex="0" data-v-6961c67e=""><a class="menu-link bix_link nav-companies" href="https://builtin.com/companies" data-v-1734c926="" data-v-6961c67e="">COMPANIES</a></li>
<li class="menu-item" tabindex="0" data-v-6961c67e=""><a class="menu-link drupal_link nav-articles" href="https://builtin.com/tech-topics" data-v-1734c926="" data-v-6961c67e="">ARTICLES&nbsp;</a></li>
<li class="menu-item" tabindex="0" data-v-6961c67e=""><a class="menu-link drupal_link nav-salaries" href="https://builtin.com/salaries" data-v-1734c926="" data-v-6961c67e="">SALARIES&nbsp;</a></li>
<li class="menu-item" tabindex="0" data-v-6961c67e=""><a class="menu-link drupal_link nav-learn" href="https://builtin.com/learn" data-v-1734c926="" data-v-6961c67e="">LEARN</a></li>
<li class="menu-item push-right" tabindex="0" data-v-6961c67e=""><a class="menu-link bix_link nav-remote" href="https://builtin.com/companies/location/fully-remote" data-v-1734c926="" data-v-6961c67e="">&nbsp;REMOTE</a></li>
</ul>
</div>
</div>
<div class="dropdown w-100 main-menu-dropdown position-absolute inactive" data-v-71fefb68="" data-v-2e21df91="" data-v-7360640c="">&nbsp;</div>
</div>
</nav></div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="region region-help">&nbsp;</div>
<div class="region region-featured-top">
<div class="region-inner">
<div id="block-blognationalheader" class="block block-bix-blogs block-bix-blogs-header-national">
<div class="block-content with-hero-image with-byline-background">
<div class="header-left">&nbsp;</div>
<div class="background">
<div class="header-right">
<div class="tags">
<div class="tags-inner"><span class="tag-name"><a href="https://builtin.com/artificial-intelligence" target="_blank">ARTIFICIAL INTELLIGENCE</a></span></div>
<div class="tags-inner"><span class="tag-name"><a href="https://builtin.com/data-science" target="_blank">DATA SCIENCE</a></span></div>
<div class="tags-inner"><span class="tag-name"><a href="https://builtin.com/expert-contributors" target="_blank">EXPERT CONTRIBUTORS</a></span></div>
<div class="tags-inner tag-inner-counter"><span class="tag-name"><span class="topic-click">+1</span></span></div>
</div>
<h1 class="title">Transformer Neural Networks: A Step-by-Step Breakdown</h1>
<div class="subtitle">The transformer neural network was first proposed in a 2017 paper to solve some of the issues of a simple RNN. This guide will introduce you to its operations.</div>
<div class="written-by-container">
<div class="writtenby_photo"><img class="b-lazy b-loaded is-b-loaded" src="https://builtin.com/cdn-cgi/image/f=auto,quality=80,width=48,height=48/https://builtin.com/sites/www.builtin.com/files/styles/64_x_64/public/2022-08/Ankit_Utkarsh.jpg" alt="" /></div>
<div class="written-info">
<div class="writtenby_name"><span class="written_by_text">Written by</span><span class="written_by_name"><a href="https://builtin.com/authors/utkarsh-ankit" hreflang="en">Utkarsh Ankit</a></span></div>
<div class="writtenby_date">Published on Jun. 28, 2022</div>
</div>
</div>
</div>
</div>
</div>
<div class="image-byline-wrapper">
<div class="blog-image"><img class="b-lazy b-loaded is-b-loaded" title="Image: Shutterstock / Built In" src="https://builtin.com/cdn-cgi/image/f=auto,quality=80,width=752,height=435/https://builtin.com/sites/www.builtin.com/files/styles/byline_image/public/2022-06/transformer-neural-network.jpg" sizes="100vw" srcset="https://builtin.com/cdn-cgi/image/f=auto,quality=80,width=320,height=185/https://builtin.com/sites/www.builtin.com/files/styles/byline_image/public/2022-06/transformer-neural-network.jpg 400w, https://builtin.com/cdn-cgi/image/f=auto,quality=80,width=500,height=290/https://builtin.com/sites/www.builtin.com/files/styles/byline_image/public/2022-06/transformer-neural-network.jpg 600w, https://builtin.com/cdn-cgi/image/f=auto,quality=80,width=752,height=435/https://builtin.com/sites/www.builtin.com/files/styles/byline_image/public/2022-06/transformer-neural-network.jpg 752w" alt="The transformer Optimus Prime points" height="435" /><a href="https://builtin.com/expert-contributors"><img src="https://builtin.com/profiles/builtin/themes/bix/assets/expert-contrib-badge.svg" alt="" /></a>
<div class="image-caption">Image: Shutterstock / Built In</div>
</div>
</div>
</div>
</div>
</div>
<a id="main-content" tabindex="-1"></a>
<div class="layout-content">
<div class="region region-content">
<div class="l-three-columns container">
<div class="l-main-container clearfix display-flex">
<div class="l-content">
<div class="row row-region-middle">
<div class="row-inside">
<div class="block-region-middle">
<div class="block block-ctools block-entity-viewnode">
<div class="updated-wrapper-blank">&nbsp;</div>
<article class="blog-national">
<div class="wrap-share-social">&nbsp;</div>
<div class="node--content">
<div class="clearfix text-formatted field field--name-body field--type-text-with-summary field--label-hidden field__item">
<p>The transformer neural network is a novel architecture that aims to solve sequence-to-sequence tasks while handling long-range dependencies with ease. It was first proposed in the paper&nbsp;&ldquo;<a href="https://arxiv.org/abs/1706.03762" target="_blank">Attention Is All You Need</a>&rdquo;&nbsp;and is now a state-of-the-art technique in the field of NLP.</p>
<p>Before jumping into the transformer network, I will explain why we use it and where it comes from. So, the story starts with RNN, which stands for recurrent neural networks.</p>
<div class="snippet-box snippet-box-text">
<div>
<h2 class="title">WHAT IS THE TRANSFORMER NEURAL NETWORK?</h2>
<div class="description">The transformer neural network is a novel architecture that aims to solve sequence-to-sequence tasks while handling long-range dependencies with ease. It was first proposed in the paper &ldquo;Attention Is All You Need.&rdquo;&nbsp; and is now a state-of-the-art technique in the field of NLP.</div>
</div>
</div>
<p class="bix-embed-read-more">MORE IN ARTIFICIAL INTELLIGENCE<a href="https://builtin.com/machine-learning/what-is-deep-learning" target="_blank">What Is Deep Learning and How Does It Work?</a></p>
<p>&nbsp;</p>
<h2>RNN</h2>
<p>What is an RNN? How is it different from a simple artificial neural network (ANN)? What is the major difference?</p>
<p>RNNs are feed-forward neural networks that are rolled out over time.</p>
<figure><img class="b-lazy updated-style-by-js b-loaded is-b-loaded" src="https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/Transformer-neural-network-1.png" alt="A schematic of an RNN" width="1000" height="267" data-entity-type="file" data-entity-uuid="81d02a2b-657a-439e-969e-d75f324012a1" />
<figcaption>Source:&nbsp;<a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank">Colah's Blog</a></figcaption>
</figure>
<p>Unlike normal neural networks, RNNs are designed to take a series of inputs with no predetermined limit on size. The term &ldquo;series&rdquo; here denotes that each input of that sequence has some relationship with its neighbors or has some influence on them.</p>
<figure><img class="b-lazy updated-style-by-js b-loaded is-b-loaded" src="https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/Transformer-neural-network-2.png" alt="A schematic of an RNN" width="1000" height="379" data-entity-type="file" data-entity-uuid="55da76bf-869c-4d73-ac23-a6271c7dc1f3" />
<figcaption>Architecture of RNNs. | Source:&nbsp;<a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank">Colah's Blog</a></figcaption>
</figure>
<p>Basic feed-forward networks &ldquo;remember&rdquo; things too, but they remember the things they learned during training. Although RNNs learn similarly during training, they also remember things learned from prior input(s) while generating output(s).</p>
<figure><img class="b-lazy updated-style-by-js" src="https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/Transformer-neural-network-3.png" alt="A schematic of an RNN" width="1000" height="330" data-entity-type="file" data-entity-uuid="dfc9ee08-f683-4e3d-ab91-6c4d3cb2d5be" />
<figcaption>Image illustrating long-term dependencies. | Source:&nbsp;<a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank">Colah's Blog</a></figcaption>
</figure>
<p>RNNs can be used in multiple types of models.</p>
<p>1. Vector-Sequence Models<strong>&nbsp;&mdash;</strong>&nbsp;Take fixed-sized vectors as input and output vectors of any size. For example, in image captioning, the image is the input and the output describes the image.</p>
<p>2. Sequence-Vector Model<strong>&nbsp;&mdash;</strong>&nbsp;Take a vector of any size and output a vector of fixed size.&nbsp; For example, sentiment analysis of a movie rates the review of any movie, positive or negative, as a fixed size vector.</p>
<p>3. Sequence-to-Sequence Model<strong>&nbsp;&mdash;</strong>&nbsp;The most popular and most used variant, this takes a sequence as input and outputs another sequence with variant sizes. An example of this is language translation for time series data for stock market prediction.</p>
<p>An RNN has two major disadvantages, however:</p>
<ol>
<li>It&rsquo;s slow to train.</li>
<li>Long sequences lead to vanishing gradient or the problem of long-term dependencies. In simple terms, its memory is not that strong when it comes to remembering old connections.</li>
</ol>
<p>For example, in the sentence&nbsp; &ldquo;The clouds are in the ____.&rdquo; the next word should obviously be sky, as it is linked with the clouds. If the distance between clouds and the predicted word is short, so the RNN can predict it easily.</p>
<p>Consider another example, however: &ldquo;I grew up in Germany with my parents, I spent many years there and have proper knowledge about their culture. That&rsquo;s why I speak fluent ____.&rdquo;</p>
<p>Here the predicted word is German, which is directly connected with Germany. The distance between Germany and the predicted word is longer in this case, however, so it&rsquo;s difficult for the RNN to predict.</p>
<p>So, unfortunately, as that gap grows, RNNs become unable to connect as their memory fades with distance.</p>
<p>&nbsp;</p>
<h2>Long Short-Term Memory</h2>
<figure><img class="b-lazy updated-style-by-js" src="https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/Transformer-neural-network-4.png" alt="A schematic of an LTSM" width="1000" height="568" data-entity-type="file" data-entity-uuid="2c5c3efa-5008-4f89-887d-ad7595df3ddf" />
<figcaption>Source:&nbsp;<a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank">Colah's Blog</a></figcaption>
</figure>
<p>Long short-term memory is a special kind of RNN, specially made for solving vanishing gradient problems. They are capable of learning long-term dependencies. In fact, remembering information for long periods of time is practically their default behaviour, not something they struggle to learn!</p>
<figure><img class="b-lazy updated-style-by-js" src="https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/Transformer-neural-network-5.png" alt="A schematic of an LTSM" width="1000" height="681" data-entity-type="file" data-entity-uuid="a738491c-e389-43c7-bd90-879316592f63" />
<figcaption>This branch allows the passing of info to skip long processing by the cell. | Source:&nbsp;<a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank">Colah's Blog</a></figcaption>
</figure>
<p>LSTM neurons, unlike the normal version, have a branch that allows passing information to skip the long processing of the current cell. This branch allows the network to retain memory for a longer period of time. It improves the vanishing gradient problem but not terribly well: It will do fine until 100 words, but around 1,000 words, it starts to lose its grip.</p>
<p>Further, like the simple RNN, it is also very slow to train, and perhaps even slower. These systems take input sequentially one by one, which doesn&rsquo;t use up GPUs very well, which are designed for parallel computation. Later, I&rsquo;ll address how we can parallelize sequential data. For now, we are dealing with two issues:</p>
<ul>
<li>Vanishing gradient</li>
<li>Slow training</li>
</ul>
<p>&nbsp;</p>
<h2>Solving the Vanishing Gradient Issue</h2>
<p>Attention&nbsp;&nbsp;answers the question of what part of the input we should focus on.&nbsp;I&rsquo;m going to explain attention via a hypothetical scenario:</p>
<p>Suppose someone gave us a book on machine learning and asked us to compile all the information about categorical cross-entropy. There are two ways of doing such a task. First, we could read the whole book and come back with the answer. Second, we could skip to the index, find the chapter on losses, go to the cross-entropy part and just read the relevant information on categorical cross-entropy.</p>
<p>Which do you think is the faster method?</p>
<p>The first approach may take a whole week, whereas the second should just take a few minutes. Furthermore, our results from the first method will be vaguer and full of too much information. The second approach will more accurately meet the requirement.</p>
<p>What did we do differently here?</p>
<p>In the former case, we didn&rsquo;t zero in on any one part of the book. In the latter method, however, we focused our attention on the losses chapter and more specifically on the part where the concept of categorical cross-entropy is explained. This second version is the way most of us humans would actually do this task.</p>
<p>Attention in neural networks is somewhat similar to what we find in humans.&nbsp;<a href="https://arxiv.org/abs/1508.04025" target="_blank">It means they focus on certain parts of the inputs while the rest gets less emphasis</a>.</p>
<p>Let&rsquo;s say we are making an NMT (neural machine translator).&nbsp;<a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" target="_blank">This animation</a>&nbsp;shows how a simple seq-to-seq model works.</p>
<p>We see that, for each step of the encoder or decoder, the RNN is processing its inputs and generating output for that time step. In each time step, the RNN updates its hidden state based on the inputs and previous outputs it has seen. In the animation, we see that the hidden state is actually the context vector we pass along to the decoder.</p>
<p>&nbsp;</p>
<h2>Time for Attention</h2>
<p>The context vector turns out to be problematic for these types of models, which struggle when dealing with long sentences. Or they may have been facing the vanishing gradient problem in long sentences. So, a solution came along in&nbsp;<a href="https://arxiv.org/abs/1508.04025" target="_blank">a paper that introduced attention</a>. It highly improved the quality of machine translation as it allows the model to focus on the relevant part of the input sequence as necessary.</p>
<p><a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" target="_blank">This attention model</a>&nbsp;is different from the classic seq-to-seq model in two ways. First, as compared to a simple seq-to-seq model, here, the encoder passes a lot more data to the decoder. Previously, only the final, hidden state of the encoding part was sent to the decoder, but now the encoder passes all the hidden states, even the intermediate ones.</p>
<p>The decoder part also does an extra step before producing its output. This step proceeds like this:</p>
<ol>
<li>It checks each hidden state that it received as every hidden state of the encoder is mostly associated with a particular word of the input sentence.</li>
<li>It give each hidden state a score.</li>
<li>Each score is multiplied by its respective softmax score, thus amplifying hidden states with high scores and drowning out hidden states with low scores. A clear visualization&nbsp;<a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" target="_blank">is available here</a>.</li>
</ol>
<p>This scoring exercise happens at each time step on the decoder side.</p>
<p>Now, when we bring the whole thing together:</p>
<ol>
<li>The attention decoder layer takes the embedding of the &lt;END&gt; token and an initial decoder hidden state. The RNN processes its inputs and produces an output and a new hidden state vector (h4).</li>
<li>Now, we use encoder hidden states and the h4 vector to calculate a context vector, C4, for this time step. This is where the attention concept is applied, giving it the name the attention step.</li>
<li>We concatenate (h4) and C4 in one vector.</li>
<li>Now, this vector is passed into a feed-forward neural network. The output of the feed-forward neural networks indicates the output word of this time step.</li>
<li>These steps get repeated for the next time steps. A clear visualization&nbsp;<a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" target="_blank">is available here</a>.</li>
</ol>
<p>So, this is how attention&nbsp;works. For further clarification, you can see its&nbsp;<a href="https://www.youtube.com/channel/UC5_6ZD6s8klmMu9TXEB_1IA" target="_blank">application to an image captioning problem here</a>.</p>
<p>Now, remember earlier I mentioned parallelizing sequential data? Here comes our ammunition for doing just that.</p>
<p class="bix-embed-read-more">MORE IN AI<a href="https://builtin.com/artificial-intelligence/automation-employee-retention" target="_blank">Why Automation Will Turn the Great Resignation Into the Great Upgrade</a></p>
<p>&nbsp;</p>
<h2>Transformers</h2>
<p>A paper called&nbsp;&ldquo;<a href="https://arxiv.org/abs/1508.04025" target="_blank">Attention Is All You Need</a>,&rdquo;&nbsp;published in 2017, introduced an encoder-decoder architecture based on attention layers, which the authors called the transformer.</p>
<p>One main difference is that the input sequence can be passed parallelly so that GPU can be used effectively and the speed of training can also be increased. It is also based on the multi-headed attention layer, so it easily overcomes the vanishing gradient issue. The paper applies the transformer to an NMT.</p>
<p>So, both of the problems that we highlighted before are partially solved here.</p>
<p>For example, in a translator made up of a simple RNN, we input our sequence or the sentence in a continuous manner, one word at a time, to generate word embeddings. As every word depends on the previous word, its hidden state acts accordingly, so we have to feed it in one step at a time.&nbsp;</p>
<p>In a transformer, however, we can pass all the words of a sentence and determine the word embedding simultaneously. So, let&rsquo;s see how it&rsquo;s actually working:</p>
<figure><img class="b-lazy updated-style-by-js" src="https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/Transformer-neural-network-11.png" alt="A schematic of the transformer neural network" width="1000" height="1445" data-entity-type="file" data-entity-uuid="4d86749c-fed2-4e7e-88ba-c5b4bc62ddf8" />
<figcaption>Source:&nbsp;<a href="https://arxiv.org/abs/1706.03762" target="_blank">arXiv:1706.03762</a></figcaption>
</figure>
<p>&nbsp;</p>
<h2>Encoder Block&nbsp;</h2>
<figure><img class="b-lazy updated-style-by-js" src="https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/Transformer-neural-network-12.png" alt="A schematic of the transformer neural network" width="1000" height="1563" data-entity-type="file" data-entity-uuid="a8ac82ca-7ee7-4714-9d44-a48c82dadb3d" />
<figcaption>Source:&nbsp;<a href="https://arxiv.org/abs/1706.03762" target="_blank">arXiv:1706.03762</a></figcaption>
</figure>
<p>Computers don&rsquo;t understand words. Instead, they work on numbers, vectors or matrices. So, we need to convert our words to a vector. But how is this possible? Here&rsquo;s where the concept of embedding space comes into play. It&rsquo;s like an open space or dictionary where words of similar meanings are grouped together. This is called an embedding space, and here every word, according to its meaning, is mapped and assigned with a particular value. Thus, we convert our words into vectors.</p>
<figure><img class="b-lazy updated-style-by-js" src="https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/Transformer-neural-network-13.png" alt="A schematic of the transformer neural network" width="1000" height="1505" data-entity-type="file" data-entity-uuid="1cd678b8-1f3e-4f1a-b3f3-ebde277f172d" />
<figcaption>Source:&nbsp;<a href="https://arxiv.org/abs/1706.03762" target="_blank">arXiv:1706.03762</a></figcaption>
</figure>
<p>One other issue we will face is that, in different sentences, each word may take on&nbsp; different meanings. So, to solve this issue, we use positional encoders. These are vectors that give context according to the position of the word in a sentence.</p>
<p>Word &rarr; Embedding &rarr; Positional Embedding &rarr; Final Vector, framed as Context.</p>
<p>So, now that our input is ready, it goes to the encoder block.</p>
<p>&nbsp;</p>
<h3>MULTI-HEAD ATTENTION PART<em>&nbsp;</em></h3>
<figure><img class="b-lazy updated-style-by-js" src="https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/Transformer-neural-network-14.png" alt="A schematic of the transformer neural network" width="1000" height="1366" data-entity-type="file" data-entity-uuid="adc83b55-391f-46b3-99c6-f74d17de8e92" />
<figcaption>Source:&nbsp;<a href="https://arxiv.org/abs/1706.03762" target="_blank">arXiv:1706.03762</a></figcaption>
</figure>
<p>Now comes the main essence of the transformer: self attention.</p>
<p>This focuses on how relevant a particular word is with respect to other words in the sentence. It is represented as an attention vector. For every word, we can generate an attention vector generated that captures the contextual relationship between words in that sentence.</p>
<p>The only problem now is that, for every word, it weighs its value much higher on itself in the sentence, but we want to know its interaction with other words of that sentence. So, we determine multiple attention vectors per word and take a weighted average to compute the final attention vector of every word.</p>
<p>As we are using multiple attention vectors, this process is called the multi-head attention block.</p>
<p>&nbsp;</p>
<h3>FEED-FORWARD NETWORK</h3>
<figure><img class="b-lazy updated-style-by-js" src="https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/Transformer-neural-network-17.png" alt="Schematic of a feed-forward network" width="1000" height="1620" data-entity-type="file" data-entity-uuid="160830b8-70b9-4f8d-b3ec-b30330965bcd" />
<figcaption>Source:&nbsp;<a href="https://arxiv.org/abs/1706.03762" target="_blank">arXiv:1706.03762</a></figcaption>
</figure>
<p>Now, the second step is the feed-forward neural network. A simple feed-forward neural network is applied to every attention vector to transform the attention vectors into a form that is acceptable to the next encoder or decoder layer.</p>
<figure><img class="b-lazy updated-style-by-js" src="https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/Transformer-neural-network-18.png" alt="A schematic of a feed-forward network" width="1000" height="1297" data-entity-type="file" data-entity-uuid="b3aa817e-2178-4b8e-b23e-532a9f930706" />
<figcaption>Source:&nbsp;<a href="https://arxiv.org/abs/1706.03762" target="_blank">arXiv:1706.03762</a></figcaption>
</figure>
<p>&nbsp;The feed-forward network accepts attention vectors one at a time. And the best thing here is, unlike the case of the RNN, each of these attention vectors is independent of one another. So, we can apply&nbsp;parallelization&nbsp;here, and that makes all the difference.&nbsp;</p>
<figure><img class="b-lazy updated-style-by-js" src="https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/Transformer-neural-network-19.png" alt="A schematic of the encoder's output." width="1000" height="2700" data-entity-type="file" data-entity-uuid="a9e5a439-a11e-4e4b-a1ab-0fcb896249ac" />
<figcaption>Encoder&rsquo;s output. | Source:&nbsp;<a href="https://arxiv.org/abs/1706.03762" target="_blank">arXiv:1706.03762</a></figcaption>
</figure>
<p>Now we can pass all the words at the same time into the encoder block and get the set of encoded vectors for every word simultaneously.</p>
<p>&nbsp;</p>
<h2>Decoder Block</h2>
<figure><img class="b-lazy updated-style-by-js" src="https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/Transformer-neural-network-20.png" alt="A schematic of the decoder block" width="1000" height="2522" data-entity-type="file" data-entity-uuid="a725b5d2-3813-40b6-ae59-ce31d4587978" />
<figcaption>Source:&nbsp;<a href="https://arxiv.org/abs/1706.03762" target="_blank">arXiv:1706.03762</a></figcaption>
</figure>
<p>Now, if we&rsquo;re training a translator for English to French, for training, we need to give an English sentence along with its translated French version for the model to learn. So, our English sentences pass through encoder block, and French sentences pass through the decoder block.</p>
<figure><img class="b-lazy updated-style-by-js" src="https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/Transformer-neural-network-21.png" alt="A schematic of the neural network" width="1000" height="2288" data-entity-type="file" data-entity-uuid="ab57bb67-2ea7-4245-b08e-61e9f2bfc5db" />
<figcaption>Source:&nbsp;<a href="https://arxiv.org/abs/1706.03762" target="_blank">arXiv:1706.03762</a></figcaption>
</figure>
<p>At first, we have the embedding layer and positional encoder part, which changes the words into respective vectors. This is similar to what we saw in the encoder part.</p>
<p>&nbsp;</p>
<h3>MASKED MULTI-HEAD ATTENTION PART<em>&nbsp;</em></h3>
<figure><img class="b-lazy updated-style-by-js" src="https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/Transformer-neural-network-22.png" alt="A schematic of the neural network" width="1000" height="2278" data-entity-type="file" data-entity-uuid="9cf32b8d-a06b-4bfe-a8a5-394c19eb7134" />
<figcaption>Source:&nbsp;<a href="https://arxiv.org/abs/1706.03762" target="_blank">arXiv:1706.03762</a></figcaption>
</figure>
<p>Now it will pass through the self-attention block, where attention vectors are generated for every word in the French sentences to represent how much each word is related to every word in the same sentence, just like we saw in the encoder part.</p>
<p>But this block is called the masked multi-head attention block, which I am going to explain in simple terms.&nbsp;First, we need to know how the learning mechanism works. When we provide an English word, it will be translated into its French version using previous results. It will then match and compare with the actual French translation that we fed into the decoder block. After comparing both, it will update its matrix value. This is how it will learn after several iterations.</p>
<p>What we observe is that we need to hide the next French word so that, at first, it will predict the next word itself using previous results without knowing the real translated word. For learning to take place, it would make no sense if it already knows the next French word. Therefore, we need to hide (or mask) it.</p>
<p>We can take any word from the English sentence, but we can only take the previous word of the French sentence for learning purposes. So, while performing parallelization with the matrix operation, we need to make sure that the matrix will mask the words appearing later by transforming them into zeroes so that the attention network can&rsquo;t use them.</p>
<figure><img class="b-lazy updated-style-by-js" src="https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/Transformer-neural-network-24.png" alt="A schematic of the neural network" width="1000" height="2323" data-entity-type="file" data-entity-uuid="69452d75-1e15-4d2b-a45a-1e565cffb30b" />
<figcaption>Source:&nbsp;<a href="https://arxiv.org/abs/1706.03762" target="_blank">arXiv:1706.03762</a></figcaption>
</figure>
<p>Now, the resulting attention vectors from the previous layer and the vectors from the encoder block are passed into another multi-head attention block. This is where the results from the encoder block also come into the picture. In the diagram, the results from the encoder block also clearly come here. That&rsquo;s why it is called the encoder-decoder attention block.</p>
<p>Since we have one vector of every word for each English and French sentence, this block actually does the mapping of English and French words and finds out the relation between them. So, this is the part where the main English to French word mapping happens.</p>
<p>The output of this block is attention vectors for every word in the English and French sentences. Each vector represents the relationship with other words in both languages.</p>
<figure><img class="b-lazy updated-style-by-js" src="https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/Transformer-neural-network-25.png" alt="A schematic of the neural network" width="1000" height="2254" data-entity-type="file" data-entity-uuid="3270d1e3-5b2b-4c7f-9631-f0f1cfdedb25" />
<figcaption>Source:&nbsp;<a href="https://arxiv.org/abs/1706.03762" target="_blank">arXiv:1706.03762</a></figcaption>
</figure>
<p>Now, if we pass each attention vector into a feed-forward unit, it will make the output vectors into a form that is easily acceptable by another decoder block or a linear layer.&nbsp;A linear layer is another feed-forward layer that expands the dimensions into numbers of words in the French language after translation.</p>
<p>Now it is passed through a softmax layer that transforms the input into a probability distribution, which is human interpretable, and the resulting word is produced with the highest probability after translation.</p>
<p>Here is an example from&nbsp;<a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html" target="_blank">Google&rsquo;s AI blog</a>.&nbsp;In the animation, the transformer starts by generating initial representations, or embeddings, for each word that are represented by the unfilled circles. Then, using self-attention, it aggregates information from all of the other words, generating a new representation per word informed by the entire context, represented by the filled balls. This step is then repeated multiple times in parallel for all words, successively generating new representations.</p>
<p>The decoder operates similarly, but generates one word at a time, from left to right. It attends not only to the other previously generated words but also to the final representations generated by the encoder.</p>
<p class="bix-embed-read-more">A MORE INTELLIGENT WORLD<a href="https://builtin.com/artificial-intelligence/ai-trust-model-governance" target="_blank">Are You Sure You Can Trust That AI?</a></p>
<p>&nbsp;</p>
<h2>The Takeaway</h2>
<p>So, this is how the transformer works, and it is now the state-of-the-art technique in NLP. Its results, using a&nbsp;self-attention mechanism, are promising,&nbsp;and it also solves the parallelization issue. Even Google uses&nbsp;<a href="https://builtin.com/data-science/using-bert-battle-job-scams" target="_blank">BERT</a>, which uses a transformer to pre-train models for common NLP applications.</p>
</div>
<div class="wrap-share-social-fixed active">&nbsp;</div>
<div class="newsletter-signup-container builtin-nuxt-app">
<section class="newsletter-signup-drupal" data-v-1a7cea8f="" data-v-26e09b2a="">
<div class="wrap-newsletter-signup" data-v-1a7cea8f="">
<div class="newsletter-signup" data-v-c0fd0ffe="" data-v-1a7cea8f="">
<div class="wrap-newsletter-signup" data-v-c0fd0ffe="">
<div class="title" data-v-1a7cea8f="" data-v-c0fd0ffe="">Subscribe to Built In to get tech articles + jobs in your inbox<span data-v-1a7cea8f="" data-v-c0fd0ffe="">.</span></div>
<div class="nl-form" data-v-c0fd0ffe="">
<div class="newsletter-signup-form" data-v-c0fd0ffe="">
<div class="nl-field nl-field-expertise is-empty" data-v-c0fd0ffe="">
<div class="field-content" data-v-c0fd0ffe=""><label for="nl-expertise-list" data-v-c0fd0ffe="">Your Expertise</label><select id="nl-expertise-list" data-v-c0fd0ffe="">
<option disabled="disabled" selected="selected" value="0" data-v-c0fd0ffe=""></option>
&nbsp;
<option value="134" data-v-c0fd0ffe="">&nbsp; &nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;Content&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;</option>
<option value="124" data-v-c0fd0ffe="">&nbsp; &nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;Data + Analytics&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;</option>
<option value="125" data-v-c0fd0ffe="">&nbsp; &nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;Design + UX&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;</option>
<option value="126" data-v-c0fd0ffe="">&nbsp; &nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;Developer + Engineer&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;</option>
<option value="123" data-v-c0fd0ffe="">&nbsp; &nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;Finance&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;</option>
<option value="127" data-v-c0fd0ffe="">&nbsp; &nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;HR + Recruiting&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;</option>
<option value="128" data-v-c0fd0ffe="">&nbsp; &nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;Legal&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;</option>
<option value="129" data-v-c0fd0ffe="">&nbsp; &nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;Marketing&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;</option>
<option value="130" data-v-c0fd0ffe="">&nbsp; &nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;Operations&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;</option>
<option value="131" data-v-c0fd0ffe="">&nbsp; &nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;Product&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;</option>
<option value="132" data-v-c0fd0ffe="">&nbsp; &nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;Project Mgmt&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;</option>
<option value="133" data-v-c0fd0ffe="">&nbsp; &nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;Sales&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;</option>
</select></div>
</div>
<div class="nl-field nl-field-email is-empty" data-v-c0fd0ffe="">
<div class="field-content" data-v-c0fd0ffe=""><label for="nl-email" data-v-c0fd0ffe="">Email Address</label><input id="nl-email" type="text" data-v-c0fd0ffe="" /></div>
</div>
<div class="nl-field nl-field-button" data-v-c0fd0ffe=""><button disabled="disabled" data-v-c0fd0ffe=""><span data-v-c0fd0ffe="">SUBSCRIBE</span></button></div>
</div>
</div>
</div>
</div>
</div>
</section>
</div>
<div class="recent-articles-container builtin-nuxt-app">
<section class="recent-articles" data-v-9015443c="" data-v-26e09b2a="">
<div class="recent-articles-wrapper" data-v-9015443c="">
<h2 data-v-9015443c="">RECENT&nbsp;<span data-v-9015443c="">ARTIFICIAL INTELLIGENCE</span>&nbsp;ARTICLES</h2>
<ul class="recent-articles-list" data-v-9015443c="">
<li class="recent-articles-card" data-v-faefd838="" data-v-9015443c=""><a class="rac-link" href="https://builtin.com/machine-learning/monte-carlo-tree-search" data-v-1734c926="" data-v-faefd838="" data-test-id="rac-link"><img class="" src="data:;base64,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 380 210"></svg>" alt="" width="380" height="210" data-v-29970e71="" data-v-faefd838="" data-v-1734c926="" /></a>
<div class="rac-title-container" data-v-faefd838="" data-v-1734c926="">
<h3 class="rac-title" data-v-faefd838="" data-v-1734c926="">Monte Carlo Tree Search: A Guide</h3>
</div>
</li>
<li class="recent-articles-card" data-v-faefd838="" data-v-9015443c=""><a class="rac-link" href="https://builtin.com/artificial-intelligence/ai-lawyer" data-v-1734c926="" data-v-faefd838="" data-test-id="rac-link"><img class="" src="data:;base64,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 380 210"></svg>" alt="" width="380" height="210" data-v-29970e71="" data-v-faefd838="" data-v-1734c926="" /></a>
<div class="rac-title-container" data-v-faefd838="" data-v-1734c926="">
<h3 class="rac-title" data-v-faefd838="" data-v-1734c926="">AI Lawyers? How Artificial Intelligence Is Transforming the Practice of Law.</h3>
</div>
</li>
<li class="recent-articles-card" data-v-faefd838="" data-v-9015443c=""><a class="rac-link" href="https://builtin.com/artificial-intelligence/automation-best-practices" data-v-1734c926="" data-v-faefd838="" data-test-id="rac-link"><img class="" src="data:;base64,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 380 210"></svg>" alt="" width="380" height="210" data-v-29970e71="" data-v-faefd838="" data-v-1734c926="" /></a>
<div class="rac-title-container" data-v-faefd838="" data-v-1734c926="">
<h3 class="rac-title" data-v-faefd838="" data-v-1734c926="">Your Business Automated. Now What?</h3>
</div>
</li>
</ul>
</div>
</section>
</div>
<div id="topics">
<div class="field field--name-field-blog-topics field--type-entity-reference field--label-hidden field__items">
<div class="field__item"><a href="https://builtin.com/artificial-intelligence" hreflang="en">Artificial Intelligence</a></div>
<div class="field__item"><a href="https://builtin.com/data-science" hreflang="en">Data Science</a></div>
<div class="field__item"><a href="https://builtin.com/expert-contributors" hreflang="en">Expert Contributors</a></div>
<div class="field__item"><a href="https://builtin.com/software-engineering-perspectives" hreflang="en">Software Engineering Perspectives</a></div>
</div>
</div>
</div>
</article>
<div class="row-inside">&nbsp;</div>
</div>
<div class="block block-bix-blogs block-bix-blogs-expert-contribution">
<div class="block-content">
<figure class="image-wrapper"><img src="https://cdn.builtin.com/profiles/builtin/themes/bix/assets/expert-contrib-badge.svg" alt="" /></figure>
<div><span class="heading">Expert Contributors</span>
<p class="expert-text">Built In&rsquo;s expert contributor network publishes thoughtful, solutions-oriented stories written by innovative tech professionals. It is the tech industry&rsquo;s definitive destination for sharing compelling, first-person accounts of problem-solving on the road to innovation.</p>
<a href="https://builtin.com/expert-contributors">LEARN MORE</a></div>
</div>
</div>
</div>
</div>
</div>
<div class="row row-region-postscript">&nbsp;</div>
<div class="row row-region-bottom">
<div class="row-inside">
<div class="block-region-bottom">
<div class="block block-bix-blogs-recruit-with-us">
<div class="block-content">
<h3>Great Companies Need Great People.&nbsp;<strong>That's Where We Come In.</strong></h3>
<a class="ga-event-processed" href="http://employers.builtin.com/membership?utm_medium=BIReferral&amp;utm_source=foremployers" data-ga-event="b2b-recruit-with-us-footer" data-builtin-track-click-event="recruit_with_us_button">RECRUIT WITH US</a></div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<footer class="main-footer-container footer-national">
<div class="footer-wrapper">
<div class="footer-content">
<div class="footer-column">
<div class="region region-footer-first">
<div id="block-footerblock" class="block block-bix-global block-bix-global-footer">
<div class="builtin-logo"><a href="https://builtin.com/"><img src="https://cdn.builtin.com/profiles/builtin/themes/bix/assets/builtin-logo.svg" alt="BuiltIn" width="105px" height="47px" /></a></div>
<div class="motto"><img src="https://cdn.builtin.com/profiles/builtin/themes/bix/assets/icons/icons/united-we-tech.svg" alt="United We Tech" width="177px" height="19px" /></div>
<div class="blurb">Built In is the online community for startups and tech companies. Find startup jobs, tech news and events.</div>
<div class="social-links">&nbsp;</div>
</div>
</div>
</div>
<div class="footer-links">
<div class="region region-footer-third"><nav id="block-about" class="block block-menu navigation menu--about">
<div id="block-about-menu" class="box-title">About</div>
<ul class="menu">
<li class="menu-item"><a href="https://builtin.com/our-story" target="_blank">Our Story</a></li>
<li class="menu-item"><a href="https://employers.builtin.com/careers/" target="_blank">Careers</a></li>
<li class="menu-item"><a href="https://builtin.com/our-staff" target="_blank">Our Staff Writers</a></li>
<li class="menu-item"><a href="https://builtin.com/content-descriptions" target="_blank">Content Descriptions</a></li>
<li class="menu-item"><a href="https://employers.builtin.com/newsroom/" target="_blank">Company News</a></li>
</ul>
</nav></div>
<div class="region region-footer-second"><nav id="block-getinvolved" class="block block-menu navigation menu--get-involved">
<div id="block-getinvolved-menu" class="box-title">Get Involved</div>
<ul class="menu">
<li class="menu-item menu-item--under-line" data-builtin-track-click-event="click_recruit_wbuiltin_footernav_link"><a href="https://employers.builtin.com/membership?utm_medium=BIReferral&amp;utm_source=foremployers" target="_blank">Recruit With Built In</a></li>
<li class="menu-item"><a href="https://builtin.com/expert-contributors" target="_blank">Become an Expert Contributor</a></li>
<li class="menu-item"><a href="https://builtin.com/send-us-tip" target="_blank">Send Us a News Tip</a></li>
</ul>
</nav></div>
<div class="region region-footer-fourth"><nav id="block-resources" class="block block-menu navigation menu--resources">
<div id="block-resources-menu" class="box-title">Resources</div>
<ul class="menu">
<li class="menu-item"><a href="https://knowledgebase.builtin.com/s/">Customer Support</a></li>
<li class="menu-item"><a href="https://form.jotform.com/223044927257054">Share Feedback</a></li>
<li class="menu-item"><a href="https://knowledgebase.builtin.com/s/contactsupport">Report a Bug</a></li>
<li class="menu-item"><a href="https://builtin.com/tech-dictionary" data-drupal-link-system-path="tech-dictionary">Tech A-Z</a></li>
<li class="menu-item"><a href="https://builtin.com/browse-jobs">Browse Jobs</a></li>
</ul>
</nav></div>
<div class="region region-footer-fifth"><nav id="block-poweredbybuiltin" class="block block-menu navigation menu--powered-by-built-in">
<div id="block-poweredbybuiltin-menu" class="box-title">Tech Hubs</div>
<ul class="menu">
<li class="menu-item"><a href="https://www.builtinaustin.com/" target="_blank">Built In Austin</a></li>
<li class="menu-item"><a href="https://www.builtinboston.com/" target="_blank">Built In Boston</a></li>
<li class="menu-item"><a href="https://www.builtinchicago.org/" target="_blank">Built In Chicago</a></li>
<li class="menu-item"><a href="https://www.builtincolorado.com/" target="_blank">Built In Colorado</a></li>
<li class="menu-item"><a href="https://www.builtinla.com/" target="_blank">Built In LA</a></li>
<li class="menu-item"><a href="https://www.builtinnyc.com/" target="_blank">Built In NYC</a></li>
<li class="menu-item"><a href="https://www.builtinsf.com/" target="_blank">Built In San Francisco</a></li>
<li class="menu-item"><a href="https://www.builtinseattle.com/" target="_blank">Built In Seattle</a></li>
<li class="menu-item"><a href="https://builtin.com/tech-hubs" target="_blank">See All Tech Hubs</a></li>
</ul>
</nav></div>
</div>
</div>
<div class="region region-footer-bottom">
<div id="block-copyrightblock" class="block block-bix-global block-bix-global-copyright">&copy; Built In 2023</div>
<nav id="block-footerbottom" class="block block-menu navigation menu--footer-bottom">
<ul class="menu">
<li class="menu-item"><a href="https://builtin.com/learning-lab-user-agreement">Learning Lab User Agreement</a></li>
<li class="menu-item"><a href="https://builtin.com/accessibility-statement">Accessibility Statement</a></li>
<li class="menu-item"><a href="https://builtin.com/copyright-policy">Copyright Policy</a></li>
<li class="menu-item"><a href="https://builtin.com/privacy-policy">Privacy Policy</a></li>
<li class="menu-item"><a href="https://builtin.com/community-terms-of-use">Terms of Use</a></li>
<li class="menu-item"><a class="do-not-sell" href="https://builtin.com/california-do-not-sell-my-information">Do Not Sell My Personal Info</a></li>
<li class="menu-item"><a href="https://builtin.com/ca-notice-collection">CA Notice of Collection</a></li>
</ul>
</nav></div>
</div>
</footer></div>
</div>